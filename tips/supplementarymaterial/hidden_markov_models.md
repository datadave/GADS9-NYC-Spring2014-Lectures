Hidden Markov Models
=================

In Hidden Markov Models, or HMM for short, the learning goal is to estimate the maximum likelihood set of conditions that produced a specific _sequence_.

There are three fundamental problems for HMMs:

* Evaluation: Given the model parameters and observed data, calculate the likelihood of the data.

* Decoding: Given the model parameters and observed data, estimate the optimal sequence of hidden states.

* Learning: Given just the observed data, estimate the model parameters.






| Title | Author | Type | Length | Difficulty | Description | Rating (1 to 4 Stars)
| ----- | ----- | ---- | ----- | ------ | --- | --- | --- |
|[HMM Lecture](http://nbviewer.ipython.org/url/iacs-courses.seas.harvard.edu/courses/am207/notebooks/Lecture18_HMM.ipynb) | Protopapas, Harvard | IPython Notebook | 6 pages | Intermediate | Great ipython notebook with thorough introduction, examples and complications. Uses SK-Learn. | ++++|
| [SK-Learn HMM](http://scikit-learn.org/stable/modules/hmm.html) | SKL | Module Documentation | 2 pages | Advanced | The documentation on HMM for SKL isn't as detailed as some others.  There are, however two examples, using HMM with stock data and a sampling demonstration as well. | ++ |

_...more to come_


